{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from bag_of_words import tf_idf_vectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_headline = pd.read_csv(os.path.join('train_stances.csv'))\n",
    "train_body = pd.read_csv(os.path.join('train_bodies.csv'))\n",
    "test_headline = pd.read_csv(os.path.join('competition_test_stances.csv'))\n",
    "test_body = pd.read_csv(os.path.join('competition_test_bodies.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_headline.join(train_body.set_index('Body ID'), on='Body ID')\n",
    "test = test_headline.join(test_body.set_index('Body ID'), on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_val(train):\n",
    "    train_subs = []\n",
    "    train_subs.append(train.query('Stance == \"agree\"'))\n",
    "    train_subs.append(train.query('Stance == \"disagree\"'))\n",
    "    train_subs.append(train.query('Stance == \"discuss\"'))\n",
    "    train_subs.append(train.query('Stance == \"unrelated\"'))\n",
    "       \n",
    "    trains = []\n",
    "    val_subs = []\n",
    "    for train_sub in train_subs:\n",
    "        length = len(train_sub)\n",
    "        if length > 0:\n",
    "            index = math.ceil(length * 0.1)\n",
    "            val_subs.append(train_sub[:index])\n",
    "            trains.append(train_sub[index:])            \n",
    "    return pd.concat(trains), pd.concat(val_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = create_val(train.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val) / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(val.query('Stance == \"agree\"')) / len(val))\n",
    "print(len(val.query('Stance == \"disagree\"')) / len(val))\n",
    "print(len(val.query('Stance == \"discuss\"')) / len(val))\n",
    "print(len(val.query('Stance == \"unrelated\"')) / len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train.query('Stance == \"agree\"')) / len(train))\n",
    "print(len(train.query('Stance == \"disagree\"')) / len(train))\n",
    "print(len(train.query('Stance == \"discuss\"')) / len(train))\n",
    "print(len(train.query('Stance == \"unrelated\"')) / len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean headlines and bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = set(stopwords.words('english'))\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    tokenized_words = []\n",
    "    for word in tokenizer.tokenize(data):\n",
    "        word = stemmer.stem(word.lower())\n",
    "        if word not in stopwords_english and word.isalpha():\n",
    "            tokenized_words.append(word)\n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences_for_row(row):\n",
    "    row['Headline'] = ' '.join(tokenize(row['Headline']))\n",
    "    row['articleBody'] = ' '.join(tokenize(row['articleBody']))\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.apply(lambda row: clean_sentences_for_row(row), axis=1)\n",
    "val = val.apply(lambda row: clean_sentences_for_row(row), axis=1)\n",
    "test = test.apply(lambda row: clean_sentences_for_row(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating tf idf matrix for all data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv(os.path.join('train_features.csv'))\n",
    "# val = pd.read_csv(os.path.join('val_features.csv'))\n",
    "# test = pd.read_csv(os.path.join('test_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_headlines = list(set(train['Headline']))\n",
    "train_bodies = list(set(train['articleBody']))\n",
    "train_sentences = train_headlines + train_bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_headlines = list(set(val['Headline']))\n",
    "val_bodies = list(set(val['articleBody']))\n",
    "val_sentences = val_headlines + val_bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_headlines = list(set(test['Headline']))\n",
    "test_bodies = list(set(test['articleBody']))\n",
    "test_sentences = test_headlines + test_bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = tf_idf_vectorizer()\n",
    "vectorizer.fit(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train_headlines = vectorizer.transform(train_headlines)\n",
    "tf_idf_train_bodies = vectorizer.transform(train_bodies)\n",
    "tf_idf_train = pd.concat([tf_idf_train_headlines, tf_idf_train_bodies])\n",
    "tf_idf_train.to_csv('tf_idf_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_val_headlines = vectorizer.transform(val_headlines)\n",
    "tf_idf_val_bodies  = vectorizer.transform(val_bodies)\n",
    "tf_idf_val = pd.concat([tf_idf_val_headlines, tf_idf_val_bodies])\n",
    "tf_idf_val.to_csv('tf_idf_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_test_headlines = vectorizer.transform(test_headlines)\n",
    "tf_idf_test_bodies = vectorizer.transform(test_bodies)\n",
    "tf_idf_test = pd.concat([tf_idf_test_headlines, tf_idf_test_bodies])\n",
    "tf_idf_test.to_csv('tf_idf_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating tf idf cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.vdot(vec1, vec2) / (LA.norm(vec1) * LA.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train = pd.read_csv(os.path.join('tf_idf_train.csv'))\n",
    "tf_idf_val = pd.read_csv(os.path.join('tf_idf_val.csv'))\n",
    "tf_idf_test = pd.read_csv(os.path.join('tf_idf_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train = tf_idf_train.set_index('sentence')\n",
    "tf_idf_val = tf_idf_val.set_index('sentence')\n",
    "tf_idf_test = tf_idf_test.set_index('sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cos_sim_features_for_row(row, tf_idf_martix):\n",
    "    headline = row['Headline']\n",
    "    body = row['articleBody']\n",
    "    headline_vec = tf_idf_martix.loc[tf_idf_martix.index==headline].values[0]\n",
    "    body_vec = tf_idf_martix.loc[tf_idf_martix.index==body].values[0]\n",
    "    row['headline_vec'] = headline_vec\n",
    "    row['body_vec'] = body_vec\n",
    "    row['tf_idf_cos_sim'] = cosine_similarity(headline_vec, body_vec)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.apply(lambda row : calc_cos_sim_features_for_row(row, tf_idf_train), axis=1)\n",
    "val = val.apply(lambda row : calc_cos_sim_features_for_row(row, tf_idf_val), axis=1)\n",
    "test = test.apply(lambda row : calc_cos_sim_features_for_row(row, tf_idf_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_features.csv', index = False)\n",
    "val.to_csv('val_features.csv', index = False)\n",
    "test.to_csv('test_features.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating LM for headline and body for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LM import Unigram\n",
    "from Interpolated_LM import interpolated_lm\n",
    "from LM import Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join('train_features.csv'))\n",
    "val = pd.read_csv(os.path.join('val_features.csv'))\n",
    "test = pd.read_csv(os.path.join('test_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_oovs(data):\n",
    "    seen = set()\n",
    "    data_with_oovs = []\n",
    "    for word in data:\n",
    "        if word in seen:\n",
    "            data_with_oovs.append(word)\n",
    "        else:\n",
    "            data_with_oovs.append('oov')\n",
    "            seen.add(word)\n",
    "    return data_with_oovs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oovs(lm, data):\n",
    "    return [word if word in lm.vocab else 'oov' for word in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(lm, data):\n",
    "    log_prob = 0.0\n",
    "    order = lm.order - 1\n",
    "    for i in range(order, len(data)):\n",
    "        word = data[i]\n",
    "        history = data[i-order:i]\n",
    "        prob = lm.probability(word, *history)\n",
    "        log_prob += math.log(prob) if prob > 0.00 else float('-inf')\n",
    "    return math.exp(-log_prob / (len(data) - order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_normalize(lm):\n",
    "    return 1 - sum([lm.probability(word) for word in lm.vocab]) <= 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(row, lm_headline, lm_body):\n",
    "    res = 0\n",
    "    order = lm_headline.order - 1\n",
    "    headline = row['Headline'].split()\n",
    "    body = row['articleBody'].split()\n",
    "    lm_headline = interpolated_lm(Unigram(headline, 0.1), lm_headline, 0.9)\n",
    "    lm_body = interpolated_lm(Unigram(body, 0.1), lm_body, 0.9)\n",
    "\n",
    "    for i in range(order, len(headline)):\n",
    "        word = headline[i]\n",
    "        history = headline[i-order:i]\n",
    "        res -= lm_headline.probability(word, *history) * math.log(lm_body.probability(word, *history))\n",
    "    row['KL_divergence'] = res\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_collection_lm(data):\n",
    "    headlines = ' '.join(list(set(data['Headline']))).split()\n",
    "    bodies = ' '.join(list(set(data['articleBody']))).split()\n",
    "    sentences = headlines + bodies   \n",
    "\n",
    "    headlines = inject_oovs(headlines)\n",
    "    bodies = inject_oovs(bodies)\n",
    "    sentences = inject_oovs(sentences)\n",
    "    \n",
    "    lm_headlines = Unigram(headlines, 0.1)\n",
    "    lm_body = Unigram(bodies, 0.1)\n",
    "    return lm_headlines, lm_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_train_headlines, lm_train_body = generate_collection_lm(train)\n",
    "lm_val_headlines, lm_val_body = generate_collection_lm(val)\n",
    "lm_test_headlines, lm_test_body = generate_collection_lm(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.apply(lambda row: KL_divergence(row, lm_train_headlines, lm_train_body), axis=1)\n",
    "val = val.apply(lambda row: KL_divergence(row, lm_val_headlines, lm_val_body), axis=1)\n",
    "test = test.apply(lambda row: KL_divergence(row, lm_test_headlines, lm_test_body), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>headline_vec</th>\n",
       "      <th>body_vec</th>\n",
       "      <th>tf_idf_cos_sim</th>\n",
       "      <th>tf_idf_eucliden_dis</th>\n",
       "      <th>tf_idf_Manhattan_dis</th>\n",
       "      <th>common_words_count</th>\n",
       "      <th>KL_divergence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>willi nelson dead internet freak anoth celebr ...</td>\n",
       "      <td>2186</td>\n",
       "      <td>agree</td>\n",
       "      <td>hoax went viral internet ha left fan one count...</td>\n",
       "      <td>[ 0.  0.  0. ...,  0.  0.  0.]</td>\n",
       "      <td>[ 0.  0.  0. ...,  0.  0.  0.]</td>\n",
       "      <td>0.251449</td>\n",
       "      <td>38.429421</td>\n",
       "      <td>298.491765</td>\n",
       "      <td>4</td>\n",
       "      <td>7.199204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heartbroken girl spend week kfc get dump</td>\n",
       "      <td>1225</td>\n",
       "      <td>agree</td>\n",
       "      <td>year old woman chengdu china southwest sichuan...</td>\n",
       "      <td>[ 0.  0.  0. ...,  0.  0.  0.]</td>\n",
       "      <td>[ 0.  0.  0. ...,  0.  0.  0.]</td>\n",
       "      <td>0.119022</td>\n",
       "      <td>53.745376</td>\n",
       "      <td>474.133136</td>\n",
       "      <td>4</td>\n",
       "      <td>7.104116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>small meteorit strike nicaragua capit citi man...</td>\n",
       "      <td>961</td>\n",
       "      <td>agree</td>\n",
       "      <td>loud boom heard saturday night resid nicaragua...</td>\n",
       "      <td>[ 0.  0.  0. ...,  0.  0.  0.]</td>\n",
       "      <td>[ 0.  0.  0. ...,  0.  0.  0.]</td>\n",
       "      <td>0.223101</td>\n",
       "      <td>31.472045</td>\n",
       "      <td>254.703593</td>\n",
       "      <td>6</td>\n",
       "      <td>4.808862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>miss jetlin rais fear new style attack libyan ...</td>\n",
       "      <td>1197</td>\n",
       "      <td>agree</td>\n",
       "      <td>u offici tuesday express fear islamist milit a...</td>\n",
       "      <td>[ 0.  0.  0. ...,  0.  0.  0.]</td>\n",
       "      <td>[ 0.  0.  0. ...,  0.  0.  0.]</td>\n",
       "      <td>0.266811</td>\n",
       "      <td>39.087583</td>\n",
       "      <td>334.983870</td>\n",
       "      <td>10</td>\n",
       "      <td>5.264663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angri mob cut rapist peni meat cleaver viewer ...</td>\n",
       "      <td>1959</td>\n",
       "      <td>agree</td>\n",
       "      <td>man hi peni cut angri mob attempt rape teenag ...</td>\n",
       "      <td>[ 0.  0.  0. ...,  0.  0.  0.]</td>\n",
       "      <td>[ 0.          0.          5.10957524 ...,  0. ...</td>\n",
       "      <td>0.307222</td>\n",
       "      <td>49.897797</td>\n",
       "      <td>483.570265</td>\n",
       "      <td>6</td>\n",
       "      <td>5.996599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID Stance  \\\n",
       "0  willi nelson dead internet freak anoth celebr ...     2186  agree   \n",
       "1           heartbroken girl spend week kfc get dump     1225  agree   \n",
       "2  small meteorit strike nicaragua capit citi man...      961  agree   \n",
       "3  miss jetlin rais fear new style attack libyan ...     1197  agree   \n",
       "4  angri mob cut rapist peni meat cleaver viewer ...     1959  agree   \n",
       "\n",
       "                                         articleBody  \\\n",
       "0  hoax went viral internet ha left fan one count...   \n",
       "1  year old woman chengdu china southwest sichuan...   \n",
       "2  loud boom heard saturday night resid nicaragua...   \n",
       "3  u offici tuesday express fear islamist milit a...   \n",
       "4  man hi peni cut angri mob attempt rape teenag ...   \n",
       "\n",
       "                     headline_vec  \\\n",
       "0  [ 0.  0.  0. ...,  0.  0.  0.]   \n",
       "1  [ 0.  0.  0. ...,  0.  0.  0.]   \n",
       "2  [ 0.  0.  0. ...,  0.  0.  0.]   \n",
       "3  [ 0.  0.  0. ...,  0.  0.  0.]   \n",
       "4  [ 0.  0.  0. ...,  0.  0.  0.]   \n",
       "\n",
       "                                            body_vec  tf_idf_cos_sim  \\\n",
       "0                     [ 0.  0.  0. ...,  0.  0.  0.]        0.251449   \n",
       "1                     [ 0.  0.  0. ...,  0.  0.  0.]        0.119022   \n",
       "2                     [ 0.  0.  0. ...,  0.  0.  0.]        0.223101   \n",
       "3                     [ 0.  0.  0. ...,  0.  0.  0.]        0.266811   \n",
       "4  [ 0.          0.          5.10957524 ...,  0. ...        0.307222   \n",
       "\n",
       "   tf_idf_eucliden_dis  tf_idf_Manhattan_dis  common_words_count  \\\n",
       "0            38.429421            298.491765                   4   \n",
       "1            53.745376            474.133136                   4   \n",
       "2            31.472045            254.703593                   6   \n",
       "3            39.087583            334.983870                  10   \n",
       "4            49.897797            483.570265                   6   \n",
       "\n",
       "   KL_divergence  \n",
       "0       7.199204  \n",
       "1       7.104116  \n",
       "2       4.808862  \n",
       "3       5.264663  \n",
       "4       5.996599  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_features.csv', index = False)\n",
    "val.to_csv('val_features.csv', index = False)\n",
    "test.to_csv('test_features.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join('train_features.csv'))\n",
    "val = pd.read_csv(os.path.join('val_features.csv'))\n",
    "test = pd.read_csv(os.path.join('test_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train = pd.read_csv(os.path.join('tf_idf_train.csv'))\n",
    "tf_idf_val = pd.read_csv(os.path.join('tf_idf_val.csv'))\n",
    "tf_idf_test = pd.read_csv(os.path.join('tf_idf_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train = tf_idf_train.set_index('sentence')\n",
    "tf_idf_val = tf_idf_val.set_index('sentence')\n",
    "tf_idf_test = tf_idf_test.set_index('sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_common_words(headline, body):\n",
    "    return sum([min(headline.count(word), body.count(word)) for word in set(headline.split())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec1, vec2):\n",
    "    return np.linalg.norm(vec1 - vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Manhattan_distance(vec1, vec2):\n",
    "    return sum(np.abs(vec1 - vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_more_features(row, tf_idf_martix):\n",
    "    headline = row['Headline']\n",
    "    body = row['articleBody']\n",
    "    headline_vec = tf_idf_martix.loc[tf_idf_martix.index==headline].values[0]\n",
    "    body_vec = tf_idf_martix.loc[tf_idf_martix.index==body].values[0]\n",
    "    row['tf_idf_eucliden_dis'] = euclidean_distance(headline_vec, body_vec)\n",
    "    row['tf_idf_Manhattan_dis'] = Manhattan_distance(headline_vec, body_vec)\n",
    "    row['common_words_count'] = count_common_words(headline, body)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.apply(lambda row : create_more_features(row, tf_idf_train), axis=1)\n",
    "val = val.apply(lambda row : create_more_features(row, tf_idf_val), axis=1)\n",
    "test = test.apply(lambda row : create_more_features(row, tf_idf_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_features.csv', index = False)\n",
    "val.to_csv('val_features.csv', index = False)\n",
    "test.to_csv('test_features.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot feature values distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join('train_features.csv'))\n",
    "val = pd.read_csv(os.path.join('val_features.csv'))\n",
    "test = pd.read_csv(os.path.join('test_features.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subs = []\n",
    "train_subs.append(train.query('Stance == \"agree\"'))\n",
    "train_subs.append(train.query('Stance == \"discuss\"'))\n",
    "train_subs.append(train.query('Stance == \"disagree\"'))\n",
    "train_subs.append(train.query('Stance == \"unrelated\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_distribution(data, column, category):\n",
    "    data[column].hist(normed=True)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Investigating the distribution of ' + column + ' for ' + str(category))\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution_for_each_category(datas, column):\n",
    "    for data in datas:\n",
    "        category = set(data['Stance'])\n",
    "        plot_distribution(data, column, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(train_subs[3], 'common_words_count', 'agree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_for_each_category(train_subs, 'common_words_count')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
